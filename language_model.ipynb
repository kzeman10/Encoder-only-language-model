{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a simple language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import urllib.request\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = \"\\033[1;34m\"\n",
    "GREEN = \"\\033[1;32m\"\n",
    "RED = \"\\033[1;31m\"\n",
    "NORMAL = \"\\033[1;30m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/174/pg174.txt\"\n",
    "local_filename = \"./data.txt\"\n",
    "\n",
    "try:\n",
    "    with urllib.request.urlopen(url) as response, open(local_filename, 'wb') as out_file:\n",
    "        data = response.read()\n",
    "        out_file.write(data)\n",
    "    print(f\"File '{local_filename}' downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download the file: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKFILEPATH = \"./data.txt\"\n",
    "CONTEXT_SIZE = 256\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Read the content of the file\n",
    "with open(BOOKFILEPATH, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "book_size = len(content)\n",
    "print(f'books size: {book_size}')\n",
    "# Create a mapping of characters to numerical IDs\n",
    "char_to_id = {char: idx for idx, char in enumerate(sorted(list(set(content))))}\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"Encode a string into a list of IDs\"\"\"\n",
    "    return [char_to_id[char] for char in text]\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"Decode a list of IDs into a string\"\"\"\n",
    "    return ''.join([list(char_to_id.keys())[list(char_to_id.values()).index(i)] for i in ids])\n",
    "\n",
    "\n",
    "# New code for sequence generator\n",
    "def sequence_generator(filepath, context_size):\n",
    "    \"\"\"reads the file and returns a generator of (context, target) pairs\"\"\"\n",
    "    buffer = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        # generate random offset\n",
    "        offset = random.randint(0, context_size // 2)\n",
    "        file.seek(offset)\n",
    "        for char in file.read():\n",
    "            buffer += [char]\n",
    "            if len(buffer) == context_size + 1:\n",
    "                yield encode(buffer[:-1]), encode(buffer[1:])\n",
    "                buffer = []\n",
    "\n",
    "\n",
    "print(f'number of unique characters: {len(char_to_id)}')\n",
    "print(char_to_id)\n",
    "assert decode(encode('hello')) == 'hello'\n",
    "print(f\"encoded hello: {encode('hello')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'about to use device: {device} device')\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\" A Transformer model with residual connections and batch normalization\"\"\"\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Apply residual connection and batch normalization before and after the transformer layer\n",
    "        src = self.norm1(src + self.transformer_encoder(src, src_mask))\n",
    "        output = self.norm2(src)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" A simple positional encoding module\"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = CONTEXT_SIZE * 2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def encode_for_model(text):\n",
    "    \"\"\"gets a string and decode it to the form that is acceptable by the model\"\"\"\n",
    "    input_data = encode(text)\n",
    "    input_data = torch.tensor(input_data)\n",
    "    input_data = input_data.reshape(-1, 1)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def generate_sequence(model, initial_sequence, new_length, temperature=1.0, top_k=12):\n",
    "    \"\"\"Generate a sequence of characters using the given model and an initial text sequence, temperature modifies the randomness of generated text\"\"\"\n",
    "    print(GREEN + initial_sequence + RED, end='')\n",
    "    sys.stdout.flush()\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for _ in range(new_length):\n",
    "            input_data = encode_for_model(initial_sequence)\n",
    "            # use only the last CONTEXT_SIZE characters as context\n",
    "            if len(input_data) > CONTEXT_SIZE:\n",
    "                input_data = input_data[-CONTEXT_SIZE:]\n",
    "            input_data = input_data.to(device)\n",
    "\n",
    "            output = model(input_data)\n",
    "            output = output[-1, :]  # take the last character from the output\n",
    "            output = output / temperature  # Apply temperature\n",
    "            # Apply top-k filtering\n",
    "            mask, _ = torch.topk(output, min(len(char_to_id), top_k), dim=-1)\n",
    "            # fill with -inf\n",
    "            output = torch.where(output < mask[:, [-1]], torch.ones_like(output, dtype=output.dtype) * -float('Inf'), output)\n",
    "            # appky softmax\n",
    "            output = F.softmax(output, dim=-1)\n",
    "            predicted_char_id = torch.multinomial(output, 1).item()\n",
    "            # predicted_char_probs = F.softmax(output[-1, -1, :] / temperature, dim=-1)  # Apply temperature\n",
    "            # predicted_char_id = torch.multinomial(predicted_char_probs, 1).item()\n",
    "\n",
    "            new_char = decode([predicted_char_id])\n",
    "            initial_sequence += new_char\n",
    "            print(new_char, end='')\n",
    "            # flush the output to see the progress\n",
    "            sys.stdout.flush()\n",
    "    print(NORMAL)\n",
    "    model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "print('preparing dataset...')\n",
    "sequence_gen = sequence_generator(BOOKFILEPATH, CONTEXT_SIZE)\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "for context, target in tqdm.tqdm(sequence_gen, total=book_size // CONTEXT_SIZE):\n",
    "    context = torch.tensor(context).reshape(-1, 1)\n",
    "    target = torch.tensor(target).reshape(-1)\n",
    "    batch_x.append(context)\n",
    "    batch_y.append(target)\n",
    "\n",
    "# stack all the batches\n",
    "batch_x = torch.stack(batch_x)\n",
    "batch_y = torch.stack(batch_y)\n",
    "print(f'batch_x shape: {batch_x.shape}')\n",
    "print(f'batch_y shape: {batch_y.shape}')\n",
    "print(f'size of the dataset: {(2 * batch_x.element_size() * batch_x.nelement() / 1024 / 1024):2f} MB')\n",
    "batch_x, batch_y = batch_x.to(device), batch_y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "def plot_loss(losses : list):\n",
    "    \"\"\"Plot the loss curve\"\"\"\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    # plt.xticks(range(len(losses)), range(len(losses)))\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('cross entropy loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Transformer model\n",
    "ntokens = len(char_to_id)  # size of vocabulary\n",
    "emsize = 40  # embedding dimension\n",
    "d_hid = 80  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 40  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder`` -- 40 max\n",
    "nhead = 40  # number of heads in ``nn.MultiheadAttention`` -- 40 max\n",
    "dropout = 0.1  # dropout probability\n",
    "\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "losses = []\n",
    "print_step = len(batch_x) // 100\n",
    "\n",
    "# if model exists, load it\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./model.pt'))\n",
    "    losses = pd.read_csv('./losses.csv').values.flatten().tolist()\n",
    "    print('model loaded...')\n",
    "except FileNotFoundError:\n",
    "    print('model not found, training from scratch...')\n",
    "    # print size of the model in MB and number of parameters\n",
    "    print(f'size of the model: {(sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024):.2f} MB')\n",
    "    print(f'number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99, verbose=True)\n",
    "\n",
    "\n",
    "# training loop\n",
    "print('training...')\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    current_time = time.time()\n",
    "    # shuffle the dataset\n",
    "    indices = torch.randperm(len(batch_x))\n",
    "    batch_x = batch_x[indices]\n",
    "    batch_y = batch_y[indices]\n",
    "    for i, (context, target) in enumerate(zip(batch_x, batch_y)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context).reshape(-1, len(char_to_id))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % print_step == 0:\n",
    "            model.eval()\n",
    "            if i != 0:\n",
    "                clear_output(wait=True)\n",
    "                losses.append(running_loss / print_step)\n",
    "                # save losses to csv\n",
    "                pd.DataFrame(losses).to_csv('./losses.csv', index=False)\n",
    "                plot_loss(losses)\n",
    "                scheduler.step()\n",
    "            print(f'progress: {(100 * i / len(batch_x)):.0f}% epoch: {epoch}/{NUM_EPOCHS} loss: {running_loss / print_step}, took: {time.time() - current_time:.2f} seconds')\n",
    "            current_time = time.time()\n",
    "            generate_sequence(model, 'As the first rays of sunlight gently illuminate the quiet ro', 30, temperature=1.1)\n",
    "            generate_sequence(model, 'The old lighthouse stood tall against the rugged coastline, ', 30, temperature=1.0)\n",
    "            generate_sequence(model, 'As the sun dipped below the horizon, John started painting t', 30, temperature=0.9)\n",
    "            print()\n",
    "            running_loss = 0\n",
    "            # save model\n",
    "            torch.save(model.state_dict(), './model.pt')\n",
    "            model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
